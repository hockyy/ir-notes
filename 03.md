# Preprocessing:

- Tokenization
- Normalization
- Stemming
- Stopwords Removal

## Memperkuat Pemahaman

Ada 3 dokumen di koleksi:

- D1: car, vacation
- D2: vacation, automobile, picnic
- D3: picnic, motorcar, river

Misal, {car, motorcar, automobile} adalah konsep yang sama. Artinya, jika user mencari dokumen yang mengandung “motorcar”, dokumen lain yang mengandung “car” dan “automobile” juga harus di-retrieve.

Bagaimana kondisi inverted index & seperti apa query processing yang dilakukan jika normalisasi terkait {car, motorcar, automobile} dilakukan dengan:

- Solusi #1 -> dengan equivalence class {car, motorcar, automobile} -> car
  - Car: 1, 2, 3
  - Picnic: 2, 3
  - River: 3
  - Vacation: 1, 2
- Solusi #2 -> query expansion
  - Q: "picnic automobile"
  - : "picnic, automobile, car, motorcar"
    - automobile: 2
    - car: 1
    - motorcar: 3
    - picnic: 2, 3
    - river: 3
    - vacation: 1, 2
- Solusi #3 -> expansion saat indexing
  - automobile: 1, 2, 3
  - car: 1, 2, 3
  - motorcar: 1, 2, 3
  - picnic: 2, 3
  - river: 3
  - vacation: 1, 2

## Normalization

- Spelling Corrections
- Spelling variations (UK, US)
- Kata kata ngga baku
- Abbreviations

## Morphology

Inflectional Morphology atau imbuhan ada yang tidak mengubah kelas kata

ada juga Derivational Morphology yang mengubah kelas kata, misalnya dari verb, ke noun, noun, ke verb, atau adjective ke noun

## Stemming & Lemmatization

Gunanya untuk mengurangi bentuk infleksi dan derivasi ke bentuk dasarnya

- Buku-buku yang berisi cerita peradaban
  - Buku yang isi cerita adab
- The boy's cars are different colors
  - The Boy car be differ color

Agar searchnya lebih toleran

- Democratic Countries
  - Democracy in Nations

Stemming:

- Crude heuristic yang ngebuang awal sama akhir
- Tidak mesti ada di kamus
- Meningkatkan recall, tapi presisinya bisa turun
  - Operate, operatig, operates -> oper

Lemmatization

- Proper process yang membuang ending infleksi dan mengembalikannya ke bentuk kamus yang merupakan sebuah kata (Lemma)
- Melibatkan kamus dan analisis morfologi
  - Operating -> operate

### Nazief & Adriani's Algorithm

```
[[[DP+]DP+]DP+] root-word [[+DS][+Possessive Pronouns][+Particles]]
```

## Removing Stop Words

Beberapa stop words ada yang penting, jadi bisa aja ga dibuang. Search engine di web biasanya udah ga pake stop list, jadi indexingnya ga mesti lambat.

## Scaling Index Construction

Sussah, karena average seek time itu lumayan tinggi, belum lagi harus transfer dari RAM memory ke CPU, serta ada block yang bisa tersebar tersebar datanya karena datanya itu virtual dan bisa aja ga berurutan di memory.

https://nlp.stanford.edu/IR-book/html/htmledition/blocked-sort-based-indexing-1.html

https://nlp.stanford.edu/IR-book/html/htmledition/single-pass-in-memory-indexing-1.html

## Distributed Indexing

![image-20220905113710361](assets/image-20220905113710361.png)

Kalau misalkan kita multi terms dia bisa ga efektif karena butuh ngakses semua databasenya, overhead can be costly

![image-20220905113739874](assets/image-20220905113739874.png)

Kalau kaya gini dia kumpulin satu satu, nanti hasilnya tinggal disatuin.

## Map-Reduce

Komputasi saat data tersebar di banyak komputer

Ada fase map yang memetakan ke key value, **termID-docID**.

Terus nanti ada fase reduce yang menggabungkan yang termidnya sama.

![image-20220905114049320](assets/image-20220905114049320.png)

### Indexing Strategy

- Single Machine
  - BSBI (Blocked sort based indexing)
    - Algorithm:
      - Segment the collection into parts of equal size
      - Sorts and group the termID-docID pairs of each part in memory
      - Store the intermediate result onto disk
      - Merge all intermediate results into the final index
    - Use termID instead of term
    - Main memory is insufficient to collect termID-docID pair, we need external sorting algorithm that uses disk
    - Collect term-docID pairs, sort them and then create postings list
    - Slower then SPIMI in terms of time complexity
    - Require to store termID, so need more space
    - Time complexity is $O(T \log T)$
  - SPIMI (Single-pass in-memory indexing)
    - Uses term instead of termID
    - write each block's dictionary to disk, and then starts a new dictionary for the next block
    - Assume we have stream of term-docID pairs
      - Tokens are processed one by one, when a term occurs for the first time, it is added to a dictionary and a new posting list is created
    - Add postings directly to the postings list
    - It is faster then BSBI because there is no Sorting being used
    - It saves memory because No termID needs to be stored
    - Time complexity $O(T)$
- Multi
  - Distributed Indexing
    - 
  - Dynamic Indexing (Bisa berubah sewaktu waktu, ada yang berkurang dan bertambah. Misalnya pada web.)
    - Tanpa adanya ausxilary index, atau in memory index, maka akan ada tambahan proses seeks yang lebih mahal

## Compression

- Menurunkan kebutuhan penyimpanan memori
- Sebagian besar indeks disimpan ke memory yang aksesnya lebih cepat
- Processing query jauh lebih cepat.

## Prediksi ukuran term vocabulary

Heap's law = $M = kT^b$

$M$ adalah banyaknya term di vocab atau dictionary dan $T$ adalah banyaknya token di collection, Jika dengan log-log plot, akan menghasilkan hubungan linier antara $M$ dan $T$.
$$
\log(M) = \log(k) + b\log(T)
$$
Biasanya $30 \leq k \leq 100$ dan $b \approx 0.5$.

Untuk dataset Reuters, itu kita pilih $k \approx 43.65$ dan $b = 0.49$

## Dictionary Compression

Biasanya di dictionary itu kita bisa pakai binary search tree, atau bisa pake hash table, intinya di dictionary itu ga linear lah. Jelas lah yakawoakwokwaoawkoaw.

Tanpa kompression, total dictionarynya menjadi:

![image-20220907114021127](assets/image-20220907114021127.png)

![image-20220907113819308](assets/image-20220907113819308.png)

Di tabel terms itu stringnya ga mesti full, bisa aja lebih dikit, pada koleksi Reuters-RCV1 pada sekitar $400\,000$ terms, dibutuhkan sekitar 28 bytes/term

 Kita bisa pakai Dictionary as A string. Misalkan untuk setiap kata rata-rata hanya 8 karakter, atau hanya 8 bytes, jadi kalau kita tempelin semua, kita pakai pointer instead.

![image-20220907113922243](assets/image-20220907113922243.png)

Jadi terkompres 

## Partisipasi (DaaS with Blocking and Non Blocking Analysis)

![image-20220907114343450](assets/image-20220907114343450.png)

## Front coding

![image-20220907141713471](assets/image-20220907141713471.png)

![image-20220907114932599](assets/image-20220907114932599.png)

