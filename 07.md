Menggunakan distance atau jarak, misal (e.g. Euclidean distance) tidak bagus dalam vector space model dalam searching.

- Proximity sepertinya lebih baik jika menggunakan sudut, daripada menggunakan jarak.
- Sudut $\rightarrow$ Cosine
- Semakin mirip vektor query dan dokumen, semakin kecil sudut antara mereka $\rightarrow$ nilai Cosine semakin besar.

![img](assets/CamScanner_09-14-2022_11.21.jpg)

<img src="assets/image-20220914112456839.png" alt="image-20220914112456839" style="zoom:33%;" />

- D1: memory, operating, system, operating, memory
- D2: memory, system
- D3: operating, operating
- D4: memory

$N = 4$ (banyak dokumen)

$DF$ itu banyak dokumen yang mengandung terms tersebut. TF untuk sebuah query akan menjadi $1 + log(TF(t, q))$ untuk setiap terms $t$ yang terdapat pada $q$. 

- $DF(memory) = 2$
- $DF(operating) = 2$

## Length Normalized Vector

Kosinus antara dua vektor lebih mudah dihitung jika keduanya sudah **length-normalized**, kalau sudah di normalized, mencari result tinggal dot product saja. Vektor dibangi dengan L2 norm atau panjangnya akan menjadi vektor unit, dokumen yang pendek dan panjang mempunyai bobot yang dibandingkan.

Jika tidak dinormalisasi,  similarity dengan dokumen yang Panjang cederung tinggi.

![image-20220914112755485](assets/image-20220914112755485.png)

### Implementasi

Pada inverted index kita bisa menyimpan beberapa value, pada setiap termsnya kita bisa simpan IDF atau bobotnya, perhatikan bahwa semakin sedikit frequency termnya muncul pada dokumen, maka nilainya semakin tinggi.

Perhatikan bahwa nanti pada postingsnya disimpan Term Frequency, tidak disimpan float karena dibutuhkan memori yang banyak.

![image-20220914113517349](assets/image-20220914113517349.png)

```python
COSINESCORE(N, q):
  Scores = [0.0] * N
  Length = [for i in posting]
  
	for t in q.unique():
		calculate w(t,q) and fetch postings list for t
		for each pair(d, tf(t,d)) in postings list do
			Scores[d] += wf(t,d) × w(t,q)

	Read the array Length[d]
	for each d do
		Scores[d] = Scores[d] / Length[d]
	return Top K components of Scores[]
```

## Tolerant Retrieval

Perlu diperbaiki karena 26% web search itu spellingnya salah.

Non-word Errors:

- graffe -> giraffe
  - graffe tidak ada di kamus

Real-word errors:

- Typographical errors
  - three -> there
- Cognitive errors
  - piece -> peace
  - too -> two
  - your -> you’re

acress:

- actress
- cress
- caress
- access
- across
- acres
