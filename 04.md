# Analisis Memori untuk Kompresi *Dictionary*

Definisikan beberapa istilah dan definisi kembali, pada koleksi Reuters-RCV1:

- Terdapat sekitar $400\,000$ terms.
- Panjang rata-rata sebuah terms (bahasa inggris) adalah $8$ karakter.

Definisikan nilai ukuran:

- Asumsikan $1$MB = $10^3$ KB = $10^6$ B
- Perhatikan bahwa $1$ byte terdiri dari $8$ bit, yang bisa merepresentasikan $2^8 = 256$ nilai berbeda secara teori.
- $1$ karakter pada sebuah string dapat direpresentasikan dengan nilai $1$ byte.

## DaaS

Perhatikan bahwa bila terdapat $400\,000$ terms, maka string panjang gabungan setiap terms kira-kira akan sepanjang $8 \frac{\text{karakter}}{terms}\times400\,000 \text{ terms} \times 1 \frac{\text{bytes}}{\text{karakter}} = 2^5 \times 10^5 \text{ bytes}$.

Perhatikan bahwa untuk string sepanjang itu, akan dibutuhkan kira-kira $\log_2{(2^5\times 10^5)} = 5 + 5\log_2(10) \approx \lceil5 + 16.6 \rceil = 22 \text{ bits}$. Karena $8$ bits itu hitungannya $1$ byte, maka akan kita sisakan $3$ byte untuk pointer si posisi term pada untaian terms yang sudah digabung.

Satu term akan membutuhkan:

- $+3$ bytes untuk alamat
- $+4$ bytes untuk frekuensi
- $+4$ bytes untuk pointer ke posting
- $+8$ bytes (rata-rata) untuk menaruhnya ke dalam untaian terms.

Sehingga total membutuhkan $19$ bytes/terms, untuk $400\,000$ terms, maka akan menggunakan $7.6 \times 10^6$ bytes, atau setara dengan $7.6$MB

## DaaS With Blocking (Tidak semua indeks dibikin alamatnya ke untaian terms, lebih tepatnya, hanya menyimpan setiap $k$-th term string)

### Analisis Memori

Perhatikan bahwa kita mula mula perlu menambahkan size dulu ke setiap saat string mulai, alasannya ialah agar bisa kita tokenize lah untaian stringnya, dan kita tahu kapan itu menjadi sebuah entri term lain saat di-*scan*.

Satu term akan membutuhkan:

- $+1$ byte untuk panjang kata
- $+8$ bytes untuk menaruhnya ke dalam untaian terms
- $+4$ untuk frekuensi
- $+4$ untuk pointer ke posting
- $+\dfrac{3}{k}$ untuk alamat. Perhatikan bahwa karena tidak semua terms akan disimpan pointernya, maka kita hanya akan menggunakan $3$ bytes saja setiap $k$ kali.

Untuk $k = 4$ :

$(17 + \dfrac{3}{k})\times 400\,000 = 7.1 \times 10^6$ bytes, yang berarti sekitar $7.1$ MB

Perhatikan bahwa:
$$
\lim_{k\to \inf} (17 + \dfrac{3}{k}) \times 400\,000 = 6.8 \text{MB}
$$
Kenapa tidak menggunakan $k$ yang besar? Karena tradeoff dengan waktu dalam mentraverse untuk mencari termsnya, sementara ukurannya tidak berubah banyak. Bahkan untuk $k = 10$, ukurannya masih sekitar $6.92$ MB. Begitu pula untuk $k = 10$.

### Analisis Waktu

Ketika tanpa blocking, pencarian di dictionary itu menggunakan binary search tree biasa. Ilustrasi berikut merepresentasikannya.

![img](assets/Notes_220907_212459.jpg)

Dengan blocking:

![img](assets/Notes_220907_213208_1.jpg)

![img](assets/Notes_220907_213208_2.jpg)

## Front Coding

Menurut saya teknik ini keren, implementasinya juga jadi menyerupai struktur data Trie, tapi memorynya in place.

![image-20220907213515085](assets/image-20220907213515085.png)

Terdapat beberapa variasi encoding untuk front coding ini, ada yang menggunakan kata sebelumnya menjadi patokan, yang menurut saya menarik, namun tentu saja pasti membutuhkan overhead dari segi waktu untuk mendecode, namun dengan tradeoff memori yang **jauh lebih kecil** bila prefixnya banyak yang dekat. Contoh di atas menurut saya cukup bagus untuk variasi kata yang prefixnya beragam.

Sumber: 

- https://www.dcs.bbk.ac.uk/~dell/teaching/nlp/dell_iir_ch05.pdf

Untuk implementasi, saya rasa kode dari repositori ini cukup bagus:

- https://github.com/WikiBox/SD