## Postings Compression

Hampir 90% penyimpanan disimpan untuk posting, dictionary dan term sebenarnya tidak terlalu besar.

<img src="assets/Screen%20Shot%202022-09-12%20at%2010.09.25.png" alt="Screen Shot 2022-09-12 at 10.09.25" style="zoom:50%;" />

Perhatikan bahwa terdapat $800\,000$ dokumen, bakal dibutuhkan sekitar $20$ bits, untuk nyimpan setiap node dari listnya. Kalau kita ingin memperkecil ukurannya, mungkin kita bisa menggunakan < 20 bits/docs id. Kompresi itu ada yang sifatnya:

- loosy, pada information retrieval, kita tidak menggunakan ini
- lossless, kompresi tanpa menghilangkan informasi, bisa kita ubah bentuknya ke data awalnya.

Entropi itu menggambarkan berapa banyak informasi yang kita simpan dalam sebuah koleksi data. Misalka ada koleksi teks $T$ yang terdiri dari array kata kata $w_1, w_2, \dots$, itu dapat dihitung menggunakan
$$
H(T) = - \sum_{w\in T} \frac{f_w}{N(T)} \log_2 ( \frac{f_w}{N(T)})
$$
Contoh: “the more dilligent the more success the better”

- freq(the) = 3
- freq(more) = 2
- freq(diligent) = freq(success) = freq(better) = 1

Yang frekuensinya lebih tinggi, harus menggunakan sedikit bit, yang jarang muncul boleh besar untuk menandai ke-perbedaannya.
$$
H(T)=-\frac{3}{8} \log _2\left(\frac{3}{8}\right)-\frac{2}{8} \log _2\left(\frac{2}{8}\right)-\frac{1}{8} \log _2\left(\frac{1}{8}\right)-\frac{1}{8} \log _2\left(\frac{1}{8}\right)-\frac{1}{8} \log _2\left(\frac{1}{8}\right)=2.055 \text { bits }
$$
![image-20220912102156453](assets/image-20220912102156453.png)

## List of Gaps Compression

![image-20220912102250364](assets/image-20220912102250364.png)

### Variable Byte Compression

1 byte yang terdiri dari 8 bit kita jadikan 1 continuation bit, sama 7 data bit.

![image-20220912102441599](assets/image-20220912102441599.png)

The bigger number you need to represent, the more byte you need., jadi menjadi list of bytes, kalau misalkan tereakhirnya 1, berarti itu package terakhir, berarti nanti dia bakal butuh byte jumlahnya itu jadi $\lceil\dfrac{\text{needed bits}}{7}\rceil$, instead of dibagi $8$.

Kita tidak kompres semuanya setelah digabung, tapi kita kompres dulu setiap postingsnya baru kita gabungkan, alasannya adalah saat kita decompress, kita tidak perlu mengdecompress semuanya.

![Screen Shot 2022-09-12 at 10.37.27](assets/Screen%20Shot%202022-09-12%20at%2010.37.27.png)

### OptPForDelta Compression

![image-20220912105412299](assets/image-20220912105412299.png)

Content

```
001 010 100 100 101 110 111 
```



Kelompokan $K$ buah gaps dan encode dengan fixed-number of bits b. Encode angka yang $> 2^b$ secara terpisah sebagai sebuah exception. Bagaimana cara pilih $b$? pilih agar kira-kira untuk setiap group ada sekitar kira-kira $10\%$ exceptions.

![image-20220912105500738](assets/image-20220912105500738.png)

![image-20220912105507707](assets/image-20220912105507707.png)

## Scoring, Term Weighting, & Vector Space Model

Pada boolean query, resultnya antara banyak sekali atau dikit sekali, beberapa pengguna lebih memilih free text queries dibandingkan boolean queries, dan ingin yang teratas itu yang paling berguna.

Jika ukuran ranking ada banyak, tampilkan top-10 dokumen yang paling relevan, tidak perlu membanjiri dengan semua hasil search.

Butuh sebuah score, untuk pasangan query dokumen. $0 \leq \text{score}(Q, D) \leq 1$, semakin banyak query pada dokumen, seharusnya scorenya semakin tinggi.

Jaccard(Q, D) = $\dfrac{Q \cap D}{Q \cup D}$

- Query: how to eat pizza

- D1: eat pizza using fork and knife = $\dfrac{2}{8}$
- D2: how to eat while coding = $\dfrac{3}{6}$

Jaccard tidak memperhatikan frekuensi data, dan bisa salah, karena dia pakai himpunan, dan bobot untuk setiap term juga bisa membuatnya beda.

Ada pula scoring lain:

score(Q, D) = $\sum_{t \in Q \cap D} w(t, D)$

dengan $w(t, D)$ adalah bobot term $t$ yang bergantung pada dokumen $D$.

Misal: $w(t, D) = TF(t, D)$ 

Tapi tidak selalu sebenarnya, relevans dari sebuah dokumen itu konsepnya tidak selalu sama standarnya dan tidak selalu meningkat secara proporsional dengan banyak nya term dalam sebuah dokumen $TF(t, D)$.

Jika $TF(t, D_A) = 10TF(t, D_B)$, bukan berarti dokumen A 10 kali lebih relevan dibanding dokumen B.

## Varian dari TF - Sublinear TF Scaling

$$
w(t, D) = 
\begin{cases}
1 + log_{10}(TF(t, D)) & TF(t, D) > 0 \\
0 & \text{otherwise}
\end{cases}
$$

$$
\text{score}(Q, D) = \sum_{t \in Q \cap D}(1 + log_{10}(TF(t, D)))
$$

Jika munculnya 1000 kali, tapi masih masuk akal karena 3 kali lebih relevan, teknik ini mendampen nilai yang terlalu besar dan tidak masuk akal.

### Bag of Words Model

Sejauh ini, posisi dari kemunculan term di dokumen tidak penting, yang penting hanyalah termnya, jadi semantic tidak diperhatikan. Sementara, meskipun semantic sebenarnya mempengaruhi relevansi.

- D1: Mary is quicker than John

- D2: John is quicker than Mary

### Inverse Document Frequency (IDF)

Tadi kita mencari itu istiahnya term itu bobot "kepentingannya" sama. Perhatikan bahwa terms yang langka itu lebih informatif. Misalnya kita pengen ngequery

> kernel, memory, and the operating systems.

- "the" itu ga informatif karena banyak banget munculnya, contoh lainnya itu **stop words**
- Kata "interrupts" itu sangat penting, karena munculnya sedikit.

![image-20220912112053519](assets/image-20220912112053519.png)

**Informativeness sebuah term berbanding terbaik dengan Document Frequency (DF)**!!!
$$
IDF(t) = \log_{10}(\frac{N}{DF(t)})
$$
$N$ adalah total banyaknya dokumen di koleksi; $DF(t)$ adalah banyaknya dokumen di koleksi yang mengandung $t$.

$\log$ perlu lagi lagi untuk mendampen nilai yang terlalu besar, misal bila $DF(t) = 1$, dengan $DF(t) = 2$, itu kan bedanya dikit, tapi bisa aja itu selisih dari $\frac{N}{DF(t)}$ nya beda jauh.

| Term   | DF      | N/DF  | log(N/DF) |
| ------ | ------- | ----- | --------- |
| Animal | 100     | 10000 | 4         |
| Sunday | 1000    | 1000  | 3         |
| Fly    | 10000   | 100   | 2         |
| Under  | 100000  | 10    | 1         |
| the    | 1000000 | 1     | 0         |

![image-20220912112946575](assets/image-20220912112946575.png)

### Vector Space Model

- D1: memory, operating, system, operating, memory
- D2: memory, system
- D3: operating, operating
- D4: memory

Perhatikan bahwa Pembobotan untuk setiap dokumen itu merupakan vektor yang berada dalam suatu ruang vektor dengan dimensi $|V|$.

![image-20220912113512004](assets/image-20220912113512004.png)

Query juga berada di *space* yang sama.

- Q = operating system
- $\vec{V}(Q) = (0 \ \ 0.12 \ \ 0.30)$

Ini 
